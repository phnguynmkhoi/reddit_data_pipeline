id,title,selftext,author,num_comments,upvote_ratio,score,created_utc,over_18,url
1fwv29o,DS to DE,"Last time I shared my article on SWE to DE, this is for Data Scientists friends.


Lot of DS are already doing some sort of Data Engineering but may be in informal way, I think they can naturally become DE by learning the right tech and approaches.


What would you like to add in the roadmap?


Would love to hear your thoughts?



If interested read more here: https://www.junaideffendi.com/p/transition-data-scientist-to-data?r=cqjft&utm_campaign=post&utm_medium=web
",mjfnd,36,0.81,150,2024-10-05 17:03:16,False,https://i.redd.it/z3sh8qmnyysd1.png
1fwpna7,"Colleague codes in Google Docs and Sheets and does not believe in source control, causing conflict. Requesting advice.","Now that I have your attention, let me give a bit of context.

My team is responsible for validating a data migration. The schemas in the source and target systems are different and there are some other complicating factors, so the whole project is quite intricate. Team member 1 (T1) was responsible for writing a script to carry out part of this validation automatically. They were writing this script based on consultations with the data engineers and software engineers working on the migration.


Then the department head announced a big reorganization. T1 would be moved to another team under the same department, while two people from another team (T2 and T3) would join my team. T1 said they would finish their script before leaving the team, and train myself, T2, and T3 on how to run it and interpret the results.


However, things did not go so smoothly. As we began the training sessions, T1 told us that they hadn't been able to finish the script in time, and that they would explain to us how it works and how to finish it. T1 would do some parts, while T2, T3, and myself would handle other parts.


The first issue was that it was very difficult to follow the training sessions. T1 is just not good at explaining things. They are verbose and unclear, and so is their documentation. Their code is also very difficult to follow. The other issue is that a lot of the details of the migration only exist in T1's head. Their justification for this or that is often ""well I had a conversation with this engineer about it."" So it's hard to ascertain the reasoning behind many parts of the script, which then makes it impossible to finish it. T2, T3, and other colleagues have agreed with me on these points.


As the training sessions continued day after day, T1 would get increasingly snippy and passive aggressive with us when we asked questions. Put simply: it was not a positive learning environment.


Things really came to a head on Friday though. T1 has an unusual approach to developing their script. T1 keeps a master copy of the script on several tabs in a Google Sheet. When part of the script needs to be changed, T1 copies that part out into a Google Doc interspersed with instructions (the instructions aren't code or comments). Then T1 reviews the Google Docs, tests the code from them, copies chunk by chunk around the instructions, and pastes them back into the Google Sheet. T1 was having us follow this method to finish the script.


I think this approach is absolutely nuts. It's not reasonable to have 4 people working on a program without some form of version control in place, and Google Docs/Sheets are not good places for writing code. I copied the code from the Sheet into a GitHub repo and added T2 and T3 to it. 


I reached out to T1 and explained my position. T1 asked to talk to T2 and I on the phone. T1's view is that source control isn't appropriate for creating new code, only for maintaining existing code, and that it would only slow us down. ""I know what source control is. Check in, check out... yeah that's going to take forever.""


T1 also doesn't see an issue with coding in Google Docs/Sheets. I disagreed. T1 then got super passive aggressive and basically said they were going to stop helping to finish the script completely and focus on their new job.


I brought this up with my manager and explained everything. They agreed with me and are escalating to the department head. At this point I really don't want to work with T1 anymore. I would rather they finish the script on their own, or me, T2, and T3 do it on our own. The issue with the latter option is that the code is so difficult to follow, and so much of the knowledge to finish it only exists in T1's head, that I think we would have to start from scratch.


I realize this is a very long post, so thanks for reading this far. Has anyone here dealt with a similar situation and have any advice?",YIRS,24,0.8,34,2024-10-05 12:49:24,False,https://www.reddit.com/r/dataengineering/comments/1fwpna7/colleague_codes_in_google_docs_and_sheets_and/
1fwr0v9,How to build a career in Data Engineering without proper education in my CV?,"I would like to ask for some career advice as I see many clever people here. 
Some time ago I made a change in my career path and became a data analyst. At first I worked only with Excel, then I moved to another company to fully work with Power BI with dataset built on the basis of Azure SQL DB. I learnt a lot about BI concepts, SQL and data fundamentals for Azure in general, improved my Power Query skills. 

The problem that I currently have is what to do next. With my knowledge and several certifications I was applying for a senior role both in my company and outside but with no success. I know it's difficult moment on the job market and my more experienced friends encourage my to keep trying saying I have a great CV for this industry.

But I also know that usually talent aquisitions teams reject people with no proper educational background, especially for senior roles. The fun fact is 3 years ago when I changed my job, I received many great offers. Now, when my CV is much better there is no interest in my applications.

My long term goal is to become a data engineer. I learnt ADF, SPSS and  some Azure tools and they are pretty cool. Now I am learning Python for data analysis and I am thinking about PCAD certificate, but sometimes I wonder if it's a good idea? What would advice to people like me at the career crossroads?

Thank you in advance for all comments. ",WineTerminator,17,0.81,32,2024-10-05 13:59:00,False,https://www.reddit.com/r/dataengineering/comments/1fwr0v9/how_to_build_a_career_in_data_engineering_without/
1fwz7qg,“Best” way to iterate over a list,"Hello, I am working on a task to copy some data from A to B. I am using PySpark for this task. The script is very simple. I have to iterate over a list that contains two info, source and destination. So what I do is a for loop to iterate through this list and for each row get the source info and copy data to a specific location. Right now it takes around 1h to complete the job. There are over 200 tables to copy and I am expecting the number will increase in the near future. Can someone please share any idea or suggestion that I can use to improve the performance. I looked into doing some parallel runs, but I couldn’t figure out how to get it to work using PySpark. Thanks a lot. ",stock_daddy,21,0.87,20,2024-10-05 20:12:15,False,https://www.reddit.com/r/dataengineering/comments/1fwz7qg/best_way_to_iterate_over_a_list/
1fwqxdq,How do mix non-Docker tools and a Docker tools in the same pipeline?,"Hiya,

I'm new to Docker and learning it since many DE tools require Docker. My current project has three parts, one block for ingestion, one for transformation, Power BI desktop, and using a Postgres db.



Important specs:

- dlt: installed in Python venv in Windows folder  
- dbt: installed in Python venv in Windows folder  
- Power BI: fully standalone in Windows folder  
- Postgres db: local install in Windows folder



I like to learn Airflow and the docs says it requires Docker. So I installed WSL2, Ubuntu, and Docker Desktop. For homework I watched the Docker Crash Course video by Techworld with Nana, which explains what Docker is, why it exist, and how it works ... but solely on Docker itself, not as part of the larger architecture.

https://preview.redd.it/lbf49b39zxsd1.png?width=1275&format=png&auto=webp&s=3d5e8332e019be19b56ff97c8cbda45b9b021727



The question is: **how do I use Airflow in this situation?** I spin it up in a container and it flawlessly 'connects' with dlt and dbt?

From what I've learned so far, I need to take into account that Airflow runs in the Docker container, which runs in WSL2/Ubuntu, correct? So it has to run the orchestration 'across' OS from Ubuntu to Windows to 'pick up/read' the dlt and dbt files? Is this the thought in the right direction?

Do you see potential issues with this, or recommend a better solution?

Thank you.

PS: it's a personal project so no business critical stuff or stakeholders I have to convince ;-)",SquidsAndMartians,12,0.92,17,2024-10-05 13:53:58,False,https://www.reddit.com/r/dataengineering/comments/1fwqxdq/how_do_mix_nondocker_tools_and_a_docker_tools_in/
1fx3dmy,"As a data analyst, am I on the right track on becoming a DE with the tools I’m using?","Currently, I’m working as a contractor data analyst, and my main tool at work is Alteryx, SQL, Excel, and VBA. we’re trying to convert VBA macros that someone else made into some Alteryx workflows. 

although, I haven’t really seen Alteryx mentioned anywhere (to my knowledge) on job postings, even on other data analyst jobs. will this somehow hurt me when applying to data engineering jobs because it’s not relevant to Data Engineering…?

also, I’m using SQL in my job a LOT by understanding my companies databases and I’m generating queries. so it seems like that will help. I’m guessing I’ll just have to keep sharpening other skills (i.e. programming, data warehouses, cloud platforms) ? any advice you can give or just letting me know if I’m on the track would be helpful :) thank you!",Lazy-Blacksmith7973,8,0.94,14,2024-10-05 23:34:54,False,https://www.reddit.com/r/dataengineering/comments/1fx3dmy/as_a_data_analyst_am_i_on_the_right_track_on/
1fwm1kl,What are some interesting startups or companies in scale-up phase in data engineering you are watching out for?,"What are some interesting startups or companies in scale-up phase in data engineering you are watching out for?

These could be companies offering interesting products or solutions and have the potential to go big over the coming years?",FrontNet3601,3,0.85,13,2024-10-05 08:47:35,False,https://www.reddit.com/r/dataengineering/comments/1fwm1kl/what_are_some_interesting_startups_or_companies/
1fwz92r,Is data normalization needed in 2024? How much normalization is actually required and when should it be done?,Can you provide some examples?,Notalabel_4566,16,0.67,11,2024-10-05 20:13:58,False,https://www.reddit.com/r/dataengineering/comments/1fwz92r/is_data_normalization_needed_in_2024_how_much/
1fx6vaq,How much documentation is too much documentation?,"I recently joined a huge company where they have way more departments than one could count. Due to the sheer size of the data each of them handles, each of them has its own data engineering team. One thing I noticed is that even though there are a lot of great engineers in the department, a lot of the reliance is on them knowing things rather than documentation (which is there but I feel wasn't enough). 

As I am training, given the complexity of the pipeline, I feel like just writing/saying that data moves from source, to here and then destination isn't enough. Am I being stupid, or are data engineers expected to just look at 100s of queries and just expected to understand/know all of it from reading those queries alone. 

I ask this as someone who wants to know, if it's my skills that are lacking(and which ones) and I should be improving on my end or documentation that show exactly how data is processed is necessary.",iCodeDayAndNight,10,0.79,8,2024-10-06 02:46:31,False,https://www.reddit.com/r/dataengineering/comments/1fx6vaq/how_much_documentation_is_too_much_documentation/
1fwte67,Build your Medallion-based Lakehouse using Spark ,"The Medallion architecture is one of the most popular architectures recommended for modern Lakehouse. How do we apply common data engineering transformations, like data cleansing and enrichment expected in Medallion architecture's Silver zone? How do we build dimensional models based on Kimball's methodology? How do we implement Slowly Changing Dimensions and surrogate keys using Microsoft Fabric's Spark notebooks? Watch this end-to-end PySpark tutorial to get the answers to these and other questions:https://youtu.be/pXCqDM24N3Y

",Nice_Substance_6594,0,0.58,4,2024-10-05 15:49:17,False,https://www.reddit.com/r/dataengineering/comments/1fwte67/build_your_medallionbased_lakehouse_using_spark/
1fwrs17,Data specific System Design,Hello! Do you have any recommendations for YouTube channels focusing on data engineering specific system design? ,chinaramr,2,0.81,6,2024-10-05 14:35:16,False,https://www.reddit.com/r/dataengineering/comments/1fwrs17/data_specific_system_design/
1fx4ugg,Quick Question: How do you guys manage PII in dimensional modelling? do you have extra dimensional tables for RBAC? or a different solution,"I am new to the world of data engineering, and have been staffed on a project where I am doing some preliminary dimensional modelling where I need to create a fact table for a machine learning model to be used by data scientists for exploration. The fact table needs to have the information of transactions, orders, customers. While for DS name itself are not important but same table is to be ised by BI analysts making BI views to show top customers. I wonder how it is done in industry. I have slowly changing dimensions of order 2 for customer info. I am particular curious to know how seasoned data engineers tackle the following -

1. Handling of PII
2. Data Lineage and Audit Tracking
3. Quality assurance

The project is slightly different from a typical snowflake data engineering analytics project because this one is all in AWS. 

 I apologize in advance if my question is stupid, I am a newbie ex data scientist trying to make my way into engineering through this project.",dorkmotter,5,0.81,3,2024-10-06 00:52:06,False,https://www.reddit.com/r/dataengineering/comments/1fx4ugg/quick_question_how_do_you_guys_manage_pii_in/
1fx4b32,Career advice and using DE for good,"Hello, I'm writing this because lately I've been seriously considering using my knowledge as a data engineer to help small businesses to grow, I'm tired of corporate environment and I think I can be of more use to society if a offer cheap technology services to small business owners.

How would you use your knowledge about DE to benefit society?
Can we actually improve business performance of small businesses if we created small apps to help them with their inventories, sales, accounting and other operations?

I think I'm not good enough at deploying software, so I'd love to create intuitive and easy to maintain and deploy software, so what do you suggest me to use to deploy my solutions?

Thank you for reading this, It would be good if everyone who knows enough about technology could share their knowledge to help small business owners to keep up with big corporations in the future.",ALESS885,0,1.0,6,2024-10-06 00:23:18,False,https://www.reddit.com/r/dataengineering/comments/1fx4b32/career_advice_and_using_de_for_good/
1fwz62s,Understanding the grand scheme of things,"I'm picking up foundational knowledge left and right beyond my core scripting and rundeck. What I am noticing is that there seems to be a fair amount of overlap between Databricks Workflows (with notebooks), dbt, and Apache Airflow. Can someone help me to better understand in a situation where you're building your data org from the ground floor, why you may choose one tool over another, and/or how these tools can be used synergistically?

For example, dbt brings testing, which is an advantage that I have not seen in Databricks workflows. Conversely, dbt and apache airflow seem to be very similar in terms of usage. Thanks in advance!",techinpanko,3,0.72,3,2024-10-05 20:10:15,False,https://www.reddit.com/r/dataengineering/comments/1fwz62s/understanding_the_grand_scheme_of_things/
1fwsy90,Ephemeral models in dbt core,"Fairly new to dbt core. Completely new to ephermeral models. 

Our project is not extremely complex but for some reason there are just way too many ephemeral models. I personally don't see the point behind using ephemeral. What do all think? Is it a good practice? Moreover, what could be a good way to back track data when ephemerals are used? 

On a side note, are there any tools that can help in tracing lineage in dbt core? 
",DataOnDrugs,2,1.0,6,2024-10-05 15:29:16,False,https://www.reddit.com/r/dataengineering/comments/1fwsy90/ephemeral_models_in_dbt_core/
1fx9quh,Data architecture for SME (small&mid enterprise),"Some friends want me to build them a solution to visualise data from e-commerce and operations.
John and 3 mates consult 10 companies working in Amazon and Ebay.
Peter works in a company with 30 employees doing procurement and finops.

When does a bigger architecture make sense?
How do you handle complexity not to make them needy?
What else should I consider for the design and planning?

My ideas are:
S3 > athena > Quicksight

S3 > airflow > RDS or WH solution> dbt > Quicksight (docker for containerisation)

Ps. Normally people care about the outcome, they do not understand the importance of the backend. I am assuming they are not able to take decisions but like it or not.
",TINHO-,0,1.0,3,2024-10-06 05:49:23,False,https://www.reddit.com/r/dataengineering/comments/1fx9quh/data_architecture_for_sme_smallmid_enterprise/
1fwnrct,Ploars regex probleam.,"I have a column in Polars dataframe containing string values. I want to find a pattern that matches to, for example, ""status"":\[1,""Open""\] (this whole thing is preceded and followed by other characters\], and to extract the number 1. But my code (given below) fails with a regex parse error.

\`\`\`python

    df = df.with_columns(
                pl.when(
                    pl.col(""data"").is_not_null()
                    & (pl.col(""data"") != """")
                    & pl.col(""data"").str.contains('""status"":[')
                )
                # Extract reges like {""status"":[1, ""Open""]} from the col
                .then(pl.col(""data"").str.extract('""status"":\\[([0-9]+),'))
                .alias(""status"")
            )
    ```

 Polars is of  version 1.9.

The error is ...

\`\`\`python

ComputeError: regex error: regex parse error:  
   ""status"":\[  
\^  
error: unclosed character class

\`\`\`

  
Anyone can throw a light on this? I tried different ways like \\""status\\"":\\\\(\[\\d+), etc. Tried with triple escape sequences \\\\\\ and quadruple like \`\`\`\`.",parijath,3,0.67,2,2024-10-05 10:54:49,False,https://www.reddit.com/r/dataengineering/comments/1fwnrct/ploars_regex_probleam/
1fwmn8e,Tech stack review/progression,"Hello Seniors. 

I am a Civil Engineer turned Data Engineer and have 4years of Analytics experience using SQL and Python.

I would like to ask for any thoughts and recommendations on my current tech stack and where i could progress in upskilling myself especially in terms of spark/scala etc2 and how i could incorporate these technologies in my current tech stack. I am also trying to find a freelance job with my current tech stack and wondering if this is enough to be of demand

Any help would be appreciated. Thank you!

https://preview.redd.it/fnlrhjnmpwsd1.png?width=1145&format=png&auto=webp&s=06833b9a80c3709486f8c29ca9d6e866fcc3c5f3

",Mysterious-Ebb1593,6,0.73,5,2024-10-05 09:33:24,False,https://www.reddit.com/r/dataengineering/comments/1fwmn8e/tech_stack_reviewprogression/
1fx3rzj,Using Informatica Power Center: easy way to load 1000+ tables from source to target?,"Ps. I am looking for a scheduled job not a one time load.
I am currently using informatica power center in a data management company I am working for. I am tasked with loading more than a 1000 tables from a source (DB2 database) to a target staging area (Oracle database).  
I am used to creating independent mappings for each table even though the only column added (modification in target table) is a reference date column. However, are there any shortcuts to do this i.e. 1 single mapping that loops (somehow) over different parameters representing sources and targets.  
Moreover, in the workflow manager i will have a 1000+ sessions for each table connected to each other.  
Looking for the easiest and less tedious way to do this whole process!",Certain_Succotash374,3,0.76,2,2024-10-05 23:55:11,False,https://www.reddit.com/r/dataengineering/comments/1fx3rzj/using_informatica_power_center_easy_way_to_load/
1fx8qm2,A tool to simplify data pipeline orchestration,"Hello - are there any tools or platforms out there that simplify managing pipeline orchestration - scheduling, monitoring, error handling, and automated scaling, all in one central dashboard? It would abstract all this management over a pipeline that comprises of several steps and tech - e.g. Kafka for ingestion, Spark for processing, and HDFS/S3 for storage. Do you see a need for it? ",dad1240,0,1.0,1,2024-10-06 04:41:01,False,https://www.reddit.com/r/dataengineering/comments/1fx8qm2/a_tool_to_simplify_data_pipeline_orchestration/
1fwqfws,Azure data storage speed,"Hi

I am working on a project to store Fastq files onto azure blob storage.
Currently I am using Azure storage explorer and it seems to take forever to make an upload from my computer. 
This was done only for a 5gb file. And there are going to be larger ones as well.
Looking for support on

1. How to make faster uploads to cloud?
2. How best to automate this process?

Thank you!

",Next-Nail9394,3,0.6,1,2024-10-05 13:29:57,False,https://www.reddit.com/r/dataengineering/comments/1fwqfws/azure_data_storage_speed/
1fwm3ml,Is advanced snowflake worth it?,"Apologies if my question is wierd. I just started my career as DE 2 months ago. I love sql & database administration. But unfortunately my work is all meetings. I do have lots of free time. I've created trail account in snowflake working through it's courses.

I'd like to go deep. I've downloaded 8 books and courses. Want to know everything about everything. Is this good thing?

Edit : I'm very low paid. Would like to master something that'll increase pay.",TuneArchitect,1,0.5,0,2024-10-05 08:52:02,False,https://www.reddit.com/r/dataengineering/comments/1fwm3ml/is_advanced_snowflake_worth_it/
